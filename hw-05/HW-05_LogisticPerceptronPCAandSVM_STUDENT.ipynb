{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment #5 (Individual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; Jeremy Barnby</p>\n",
    "### <p style=\"text-align: right;\"> &#9989; barnbyje</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals for this homework assignment\n",
    "\n",
    "By the end of this assignment, you should be able to:\n",
    "* Use `git` to track your work and turn in your assignment\n",
    "* Read in data and prepare it for modeling\n",
    "* Build, fit, and evaluate Logistic Regression models\n",
    "* Build, fit, and evaluate Perceptron models\n",
    "* Use PCA to reduce the number of features\n",
    "* Build, fit, and evaluate an SVC model on PCA-transformed data\n",
    "* Systematically investigate the effects of the number of PCA components on an SVC model of data\n",
    "\n",
    "### Assignment instructions:\n",
    "\n",
    "Work through the following assignment, making sure to follow all of the directions and answer all of the questions.\n",
    "\n",
    "There are **41 points** possible on this assignment. Point values for each part are included in the section headers.\n",
    "\n",
    "This assignment is **due at 11:59 pm on Friday, April 15th**. It should be uploaded into the \"Homework Assignments\" submission folder for Homework #5. Submission instructions can be found at the end of the notebook.. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Add to your Git repository to track your progress on your assignment (4 points)\n",
    "\n",
    "For this assignment, you're going to add it to the `cmse202-s22-turnin` repository you created in class so that you can track your progress on the assignment and preserve the final version that you turn in. In order to do this you need to\n",
    "\n",
    "**&#9989; Do the following**:\n",
    "\n",
    "1. Navigate to your `cmse202-s22-turnin` repository and create a new directory called `hw-05`.\n",
    "2. Move this notebook into that **new directory** in your repository, then **add it and commit it to your repository**.\n",
    "1. Finally, to test that everything is working, \"git push\" the file so that it ends up in your GitHub repository.\n",
    "\n",
    "**Important**: Double check you've added your Professor and your TA as collaborators to your \"turnin\" repository (you should have done this in the previous homework assignment).\n",
    "\n",
    "**Also important**: Make sure that the version of this notebook that you are working on is the same one that you just added to your repository! If you are working on a different copy of the notebook, **none of your changes will be tracked**!\n",
    "\n",
    "If everything went as intended, the file should now show up on your GitHub account in the \"`cmse202-s22-turnin`\" repository inside the `hw-05` directory that you just created.  Periodically, **you'll be asked to commit your changes to the repository and push them to the remote GitHub location**. Of course, you can always commit your changes more often than that, if you wish.  It can be good to get into a habit of committing your changes any time you make a significant modification, or when you stop working on the project for a bit.\n",
    "\n",
    "&#9989; **Do this**: Before you move on, put the command that your instructor should run to clone your repository in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git clone https://github.com/barnbyje/cmse202-s22-turnin/tree/main/hw-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Do this**: Before you move on, create a new branch called `hw05_branch` and move into it. In the cell below put the command(s) to create a new branch and to checkout the new branch. (_Note_: your TA will be able to see if you have created the branch and its history)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git checkout -b hw05_branch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Do this**: Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"loading\"></a>\n",
    "## Part 2. Logistic Regression (12 points)\n",
    "### 2.1 Data processing (5 points)\n",
    "For this part, you will read and process the dataset `hw5_data.csv` and split the training and testing sets.\n",
    "\n",
    "The provided data corresponds to a molecular biology dataset, where each row represents a patient classified into either \"active\" or \"repressive\". The columns represent features, where each feature comes from the quantification of a specific gene. Ten genes (ten features) are measured. The goal is to make predictive models that can classify patients (\"active\" or \"repressive\") based on the ten features.\n",
    "\n",
    "The dataset is located at:\n",
    "`https://raw.githubusercontent.com/msu-cmse-courses/cmse202-S22-data/main/data/hw5_data.csv`\n",
    "\n",
    "\n",
    "**&#9989; Question 2.1.1 (1 point):** Read the `hw5_data.csv` file into your notebook and print out the unique labels in the `label` columns. \n",
    "\n",
    "Note: each row represents one data point and each column (except the `label` column) represents one feature. The `label` column corresponds to the class labels for every data point. There are two types of unique class labels in the `label` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['active' 'repressive']\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "genes = pd.read_csv('hw5_data.csv')\n",
    "print(np.unique(genes['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 2.1.2 (1 point):** To simplify the process of data modeling, we should convert the labels from strings to integers.\n",
    "\n",
    "Replace all of the strings in your `label` column with integers based on the following:\n",
    "\n",
    "| original label | integer label |\n",
    "| -------- | -------- |\n",
    "| repressive | 0 |\n",
    "| active | 1 |\n",
    "\n",
    "Once you've replaced the labels, display your DataFrame and confirm that it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>1.089799</td>\n",
       "      <td>3.706677</td>\n",
       "      <td>1.839418</td>\n",
       "      <td>1.000414</td>\n",
       "      <td>0.751246</td>\n",
       "      <td>-0.077189</td>\n",
       "      <td>0.949589</td>\n",
       "      <td>1.641961</td>\n",
       "      <td>1.102132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.355099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.247742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194369</td>\n",
       "      <td>0.116891</td>\n",
       "      <td>-0.059497</td>\n",
       "      <td>1.086607</td>\n",
       "      <td>0.508670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.814211</td>\n",
       "      <td>2.696122</td>\n",
       "      <td>0.221476</td>\n",
       "      <td>0.229138</td>\n",
       "      <td>-0.173686</td>\n",
       "      <td>1.091221</td>\n",
       "      <td>1.048915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.690140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527291</td>\n",
       "      <td>0.185705</td>\n",
       "      <td>-0.089479</td>\n",
       "      <td>-0.379929</td>\n",
       "      <td>-0.093369</td>\n",
       "      <td>0.272125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.376770</td>\n",
       "      <td>0.631267</td>\n",
       "      <td>2.090756</td>\n",
       "      <td>1.581667</td>\n",
       "      <td>0.793976</td>\n",
       "      <td>0.846570</td>\n",
       "      <td>0.178551</td>\n",
       "      <td>0.245401</td>\n",
       "      <td>1.221811</td>\n",
       "      <td>0.111456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>1.539002</td>\n",
       "      <td>0.499277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.934047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750042</td>\n",
       "      <td>0.605361</td>\n",
       "      <td>0.436925</td>\n",
       "      <td>1.426063</td>\n",
       "      <td>0.540208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.646765</td>\n",
       "      <td>0.198733</td>\n",
       "      <td>0.605987</td>\n",
       "      <td>0.306014</td>\n",
       "      <td>0.437594</td>\n",
       "      <td>1.220802</td>\n",
       "      <td>0.908293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>0.368634</td>\n",
       "      <td>2.356287</td>\n",
       "      <td>1.937731</td>\n",
       "      <td>0.684271</td>\n",
       "      <td>1.801650</td>\n",
       "      <td>1.492007</td>\n",
       "      <td>0.463434</td>\n",
       "      <td>1.313578</td>\n",
       "      <td>0.908521</td>\n",
       "      <td>0.958253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.668051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.213437</td>\n",
       "      <td>0.403462</td>\n",
       "      <td>1.526797</td>\n",
       "      <td>0.274236</td>\n",
       "      <td>1.283074</td>\n",
       "      <td>0.795663</td>\n",
       "      <td>0.757853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>1.177402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.970539</td>\n",
       "      <td>0.119612</td>\n",
       "      <td>0.777841</td>\n",
       "      <td>-0.106966</td>\n",
       "      <td>0.368507</td>\n",
       "      <td>0.138330</td>\n",
       "      <td>0.340818</td>\n",
       "      <td>-0.378622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0        1   0.000168   1.089799   3.706677   1.839418   1.000414   0.751246   \n",
       "1        1   0.355099   0.000000   2.247742        NaN   0.000000   0.194369   \n",
       "2        1   0.001236   0.000000        NaN   0.814211   2.696122   0.221476   \n",
       "3        1   0.690140   0.000000   0.687331   0.000000   0.527291   0.185705   \n",
       "4        1   1.376770   0.631267   2.090756   1.581667   0.793976   0.846570   \n",
       "..     ...        ...        ...        ...        ...        ...        ...   \n",
       "995      0   1.539002   0.499277   0.000000   1.934047   0.000000   0.750042   \n",
       "996      0   0.004665   0.356522   0.000000   1.646765   0.198733   0.605987   \n",
       "997      0   0.368634   2.356287   1.937731   0.684271   1.801650   1.492007   \n",
       "998      0   0.000000   1.668051   0.000000   1.213437   0.403462   1.526797   \n",
       "999      0   1.177402   0.000000   1.970539   0.119612   0.777841  -0.106966   \n",
       "\n",
       "     feature_7  feature_8  feature_9  feature_10  \n",
       "0    -0.077189   0.949589   1.641961    1.102132  \n",
       "1     0.116891  -0.059497   1.086607    0.508670  \n",
       "2     0.229138  -0.173686   1.091221    1.048915  \n",
       "3    -0.089479  -0.379929  -0.093369    0.272125  \n",
       "4     0.178551   0.245401   1.221811    0.111456  \n",
       "..         ...        ...        ...         ...  \n",
       "995   0.605361   0.436925   1.426063    0.540208  \n",
       "996   0.306014   0.437594   1.220802    0.908293  \n",
       "997   0.463434   1.313578   0.908521    0.958253  \n",
       "998   0.274236   1.283074   0.795663    0.757853  \n",
       "999   0.368507   0.138330   0.340818   -0.378622  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put your code here\n",
    "genes['label'] = genes['label'].replace('repressive',0)\n",
    "genes['label'] = genes['label'].replace('active',1)\n",
    "genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 2.1.3 (1 point):** At this point, you've hopefully noticed that some of the rows seems to have missing data values as indicated by the existence of `NaN` values. Since we don't necessarily know what to replace these values with, let's just play it safe and remove all of the rows that have `NaN` in any of the column entries. This should help to ensure that we don't end up with errors or confusing results when we try to classify the data.\n",
    "\n",
    "Remove all of the rows that contain a `NaN` in any column. **Make sure you actually store this new version of your dataframe either in the original variable name or in a new variable name**. If everything went as intended, you should find that you have 793 rows left over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>1.089799</td>\n",
       "      <td>3.706677</td>\n",
       "      <td>1.839418</td>\n",
       "      <td>1.000414</td>\n",
       "      <td>0.751246</td>\n",
       "      <td>-0.077189</td>\n",
       "      <td>0.949589</td>\n",
       "      <td>1.641961</td>\n",
       "      <td>1.102132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.690140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527291</td>\n",
       "      <td>0.185705</td>\n",
       "      <td>-0.089479</td>\n",
       "      <td>-0.379929</td>\n",
       "      <td>-0.093369</td>\n",
       "      <td>0.272125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.376770</td>\n",
       "      <td>0.631267</td>\n",
       "      <td>2.090756</td>\n",
       "      <td>1.581667</td>\n",
       "      <td>0.793976</td>\n",
       "      <td>0.846570</td>\n",
       "      <td>0.178551</td>\n",
       "      <td>0.245401</td>\n",
       "      <td>1.221811</td>\n",
       "      <td>0.111456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.407187</td>\n",
       "      <td>0.590095</td>\n",
       "      <td>0.063048</td>\n",
       "      <td>0.559918</td>\n",
       "      <td>0.370942</td>\n",
       "      <td>0.084975</td>\n",
       "      <td>0.486121</td>\n",
       "      <td>0.207094</td>\n",
       "      <td>0.660959</td>\n",
       "      <td>0.326050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1.238114</td>\n",
       "      <td>1.268546</td>\n",
       "      <td>1.515984</td>\n",
       "      <td>1.174424</td>\n",
       "      <td>0.520594</td>\n",
       "      <td>1.222930</td>\n",
       "      <td>0.268151</td>\n",
       "      <td>0.983738</td>\n",
       "      <td>0.952295</td>\n",
       "      <td>0.460386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>1.539002</td>\n",
       "      <td>0.499277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.934047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750042</td>\n",
       "      <td>0.605361</td>\n",
       "      <td>0.436925</td>\n",
       "      <td>1.426063</td>\n",
       "      <td>0.540208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.646765</td>\n",
       "      <td>0.198733</td>\n",
       "      <td>0.605987</td>\n",
       "      <td>0.306014</td>\n",
       "      <td>0.437594</td>\n",
       "      <td>1.220802</td>\n",
       "      <td>0.908293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>0.368634</td>\n",
       "      <td>2.356287</td>\n",
       "      <td>1.937731</td>\n",
       "      <td>0.684271</td>\n",
       "      <td>1.801650</td>\n",
       "      <td>1.492007</td>\n",
       "      <td>0.463434</td>\n",
       "      <td>1.313578</td>\n",
       "      <td>0.908521</td>\n",
       "      <td>0.958253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.668051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.213437</td>\n",
       "      <td>0.403462</td>\n",
       "      <td>1.526797</td>\n",
       "      <td>0.274236</td>\n",
       "      <td>1.283074</td>\n",
       "      <td>0.795663</td>\n",
       "      <td>0.757853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>1.177402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.970539</td>\n",
       "      <td>0.119612</td>\n",
       "      <td>0.777841</td>\n",
       "      <td>-0.106966</td>\n",
       "      <td>0.368507</td>\n",
       "      <td>0.138330</td>\n",
       "      <td>0.340818</td>\n",
       "      <td>-0.378622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>793 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0        1   0.000168   1.089799   3.706677   1.839418   1.000414   0.751246   \n",
       "3        1   0.690140   0.000000   0.687331   0.000000   0.527291   0.185705   \n",
       "4        1   1.376770   0.631267   2.090756   1.581667   0.793976   0.846570   \n",
       "6        1   0.407187   0.590095   0.063048   0.559918   0.370942   0.084975   \n",
       "8        1   1.238114   1.268546   1.515984   1.174424   0.520594   1.222930   \n",
       "..     ...        ...        ...        ...        ...        ...        ...   \n",
       "995      0   1.539002   0.499277   0.000000   1.934047   0.000000   0.750042   \n",
       "996      0   0.004665   0.356522   0.000000   1.646765   0.198733   0.605987   \n",
       "997      0   0.368634   2.356287   1.937731   0.684271   1.801650   1.492007   \n",
       "998      0   0.000000   1.668051   0.000000   1.213437   0.403462   1.526797   \n",
       "999      0   1.177402   0.000000   1.970539   0.119612   0.777841  -0.106966   \n",
       "\n",
       "     feature_7  feature_8  feature_9  feature_10  \n",
       "0    -0.077189   0.949589   1.641961    1.102132  \n",
       "3    -0.089479  -0.379929  -0.093369    0.272125  \n",
       "4     0.178551   0.245401   1.221811    0.111456  \n",
       "6     0.486121   0.207094   0.660959    0.326050  \n",
       "8     0.268151   0.983738   0.952295    0.460386  \n",
       "..         ...        ...        ...         ...  \n",
       "995   0.605361   0.436925   1.426063    0.540208  \n",
       "996   0.306014   0.437594   1.220802    0.908293  \n",
       "997   0.463434   1.313578   0.908521    0.958253  \n",
       "998   0.274236   1.283074   0.795663    0.757853  \n",
       "999   0.368507   0.138330   0.340818   -0.378622  \n",
       "\n",
       "[793 rows x 11 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put your code here \n",
    "genes = genes.dropna(axis = 0)\n",
    "genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 2.1.4 (1 point):** As we've seen when working with `sklearn` it can be much easier to work with the data if we have separate variables: one that stores the feature matrix and one that stores the class labels.\n",
    "\n",
    "Split your DataFrame so that you have two separate DataFrames: (1) one called `features`, which contains all columns of features; and (2) one called `labels`, which is a single-column dataframe that contains all of the *new* integer labels you just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "features = genes.loc[:,'feature_1':'feature_10']\n",
    "labels = genes.loc[:,'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.1.5 (1 point):** How balanced is your dataset? You need to write a bit of code to figure out how balanced your dataset is, by counting the numbers of data points of each classe label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active: 391\n",
      "Repressive: 402\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "a = []\n",
    "b = []\n",
    "for n in labels:\n",
    "    \n",
    "    if n == 1:\n",
    "        a.append(n)\n",
    "        \n",
    "    else:\n",
    "        b.append(n)\n",
    "        \n",
    "print('Active:', len(a))\n",
    "print('Repressive:', len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> This is a pretty balanced dataset. There is only a difference of 11 observations here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2 Logistic Regression (7 points)\n",
    "\n",
    "For this part, you will apply logistic regression to tackle th classification problem: predicting class labels based on the features.\n",
    "\n",
    "**&#9989; Question 2.2.1 (1 point):** Split your data into a training and a testing set with a training set representing 75% of your data. For reproducibility , set the `random_state` argument to `314159`. Print the lengths to show you have the right number of entries for the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Training Features: 594\n",
      "# of Test Features: 199\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(features,labels,test_size = .25, random_state = 314159)\n",
    "print('# of Training Features:',len(train_vectors))\n",
    "print('# of Test Features:',len(test_vectors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 2.2.2 (3 points):** Build a Logistic regression model based on default settings.\n",
    "\n",
    "Add constant term in both training and testing features, fit Logistic regression based on the training set, and then print out the model summary.\n",
    "\n",
    "**Note:** You can use the built-in model `Logit` in `statsmodels.api`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.620529\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  label   No. Observations:                  594\n",
      "Model:                          Logit   Df Residuals:                      583\n",
      "Method:                           MLE   Df Model:                           10\n",
      "Date:                Thu, 14 Apr 2022   Pseudo R-squ.:                  0.1039\n",
      "Time:                        15:36:31   Log-Likelihood:                -368.59\n",
      "converged:                       True   LL-Null:                       -411.32\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.244e-14\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.8952      0.234     -3.830      0.000      -1.353      -0.437\n",
      "feature_1      0.5892      0.158      3.721      0.000       0.279       0.900\n",
      "feature_2      0.2239      0.427      0.525      0.600      -0.612       1.060\n",
      "feature_3      0.3242      0.072      4.475      0.000       0.182       0.466\n",
      "feature_4     -0.3273      0.332     -0.985      0.325      -0.979       0.324\n",
      "feature_5     -0.0688      0.150     -0.460      0.646      -0.362       0.224\n",
      "feature_6      0.2728      0.462      0.590      0.555      -0.633       1.179\n",
      "feature_7     -0.3599      0.437     -0.823      0.410      -1.217       0.497\n",
      "feature_8     -0.2714      0.440     -0.617      0.537      -1.133       0.590\n",
      "feature_9     -0.0728      0.435     -0.167      0.867      -0.925       0.780\n",
      "feature_10    -0.0810      0.255     -0.317      0.751      -0.581       0.419\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  x = pd.concat(x[::order], 1)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "x = sm.add_constant(test_vectors)\n",
    "y = sm.add_constant(train_vectors)\n",
    "logit_model = sm.Logit(train_labels, y)\n",
    "\n",
    "result = logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.2.3 (1 point):** What is the Pseudo R^2? Which features have p-value < 0.05?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> \n",
    "The Pseudo-R^2 value is 0.1039. This means that only 10.39% of the variability in the model is explained by the features.\n",
    "\n",
    "feature_1, feature_3 both have p-values less than 0.05. The constant does too, but I don't think we're really counting that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.2.4 (2 points):** Make predictions for the testing set using the trained model.\n",
    "\n",
    "Note: the logistic regression model predicts the probability of belonging to class 1. To make the final binary classification, let's the threshold to be 0.5, which means that every sample in the testing set with predicted probability scores greater than 0.5 will be predicted as '1', and other samples with predicted probability less than 0.5 will be predicted as '0'. \n",
    "\n",
    "Show the model's accuracy score based on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6683417085427136\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "predicted_labels = result.predict(x)\n",
    "output = [1 if x > 0.5 else 0 for x in predicted_labels]\n",
    "print('Accuracy Score:',accuracy_score(output, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 2\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. Perceptron (5 points)\n",
    "\n",
    "For this part, you will use another model, Perceptron, to continue working on the same classification problem.\n",
    "\n",
    "**&#9989; Question 3.1 (2 points):** (1) Build a Perceptron model with default settings, and fit the model based on the training set.\n",
    "\n",
    "(2) Apply the trained model on the test features to predict the labels for the testing dataset. \n",
    "\n",
    "(3) Evaluate the model by printing out the confusion matrix and classification report, based on its performance on the testing dataset.\n",
    "\n",
    "**Note:** You can use the built-in model `Perceptron` in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[230  78]\n",
      " [131 155]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69       308\n",
      "           1       0.67      0.54      0.60       286\n",
      "\n",
      "    accuracy                           0.65       594\n",
      "   macro avg       0.65      0.64      0.64       594\n",
      "weighted avg       0.65      0.65      0.64       594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "from sklearn import svm\n",
    "\n",
    "X,y = train_vectors, train_labels\n",
    "\n",
    "cls = svm.SVC(kernel=\"linear\", C=10)\n",
    "cls.fit(X,y)\n",
    "\n",
    "y_predicted = cls.predict(X)\n",
    "\n",
    "print(confusion_matrix(y,y_predicted))\n",
    "print(classification_report(y,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 3.2 (3 points):**. Finding the best penalty term.\n",
    "\n",
    "`Perceptron` from the `sklearn` can employ different penalty terms, including `l1`, `l2`, and `elasticnet` (Note: check the `penalty` argument of `Perceptron`). Apply the Perceptron on the training dataset again, based on different penalty terms (i.e. make 3 Perceptron models). Print out the accuray score of each model, based on the testing dataset. \n",
    "\n",
    "Which penalty term results in the best accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalty = l1\n",
      "[[168 140]\n",
      " [ 82 204]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.55      0.60       308\n",
      "           1       0.59      0.71      0.65       286\n",
      "\n",
      "    accuracy                           0.63       594\n",
      "   macro avg       0.63      0.63      0.62       594\n",
      "weighted avg       0.63      0.63      0.62       594\n",
      "\n",
      "Accuracy score: 0.6262626262626263\n",
      "Penalty = l2\n",
      "[[177 131]\n",
      " [ 88 198]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.57      0.62       308\n",
      "           1       0.60      0.69      0.64       286\n",
      "\n",
      "    accuracy                           0.63       594\n",
      "   macro avg       0.63      0.63      0.63       594\n",
      "weighted avg       0.64      0.63      0.63       594\n",
      "\n",
      "Accuracy score: 0.6313131313131313\n",
      "Penalty = elasticnet\n",
      "[[226  82]\n",
      " [138 148]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.73      0.67       308\n",
      "           1       0.64      0.52      0.57       286\n",
      "\n",
      "    accuracy                           0.63       594\n",
      "   macro avg       0.63      0.63      0.62       594\n",
      "weighted avg       0.63      0.63      0.62       594\n",
      "\n",
      "Accuracy score: 0.6296296296296297\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "model_l1 = Perceptron(penalty = 'l1', random_state = 1)\n",
    "model_l1.fit(X,y)\n",
    "l1_y_predicted = model_l1.predict(X)\n",
    "print('Penalty = l1')\n",
    "print(confusion_matrix(y,l1_y_predicted))\n",
    "print(classification_report(y,l1_y_predicted))\n",
    "print('Accuracy score:',accuracy_score(y, l1_y_predicted))\n",
    "\n",
    "model_l2 = Perceptron(penalty = 'l2', random_state = 1)\n",
    "model_l2.fit(X,y)\n",
    "l2_y_predicted = model_l2.predict(X)\n",
    "print('Penalty = l2')\n",
    "print(confusion_matrix(y,l2_y_predicted))\n",
    "print(classification_report(y,l2_y_predicted))\n",
    "print('Accuracy score:',accuracy_score(y, l2_y_predicted))\n",
    "\n",
    "model_elas = Perceptron(penalty = 'elasticnet', random_state = 1)\n",
    "model_elas.fit(X,y)\n",
    "elas_y_predicted = model_elas.predict(X)\n",
    "print('Penalty = elasticnet')\n",
    "print(confusion_matrix(y,elas_y_predicted))\n",
    "print(classification_report(y,elas_y_predicted))\n",
    "print('Accuracy score:',accuracy_score(y, elas_y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model with the L2 penalty is the most accurate, with an accuracy of 63.13%. I printed the confusion matrix and classification report as well just to point out to myself why producing the accuracy score is important. It is hard to get a good grasp on a model's accuracy just by looking at the confusion matrix and classification report. The L1 model produced 222 misclassified objects, while the elasticnet model produced 220 misclassifications. The L2 model produced slightly less at 219 misclassifications. All these models produce pretty similar results, but the L2 model performs just a little better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 3\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4. Principal Component Analysis (6 points)\n",
    "\n",
    "The full model uses all 10 features to predict the results. In many cases, we might need to see how close we can get with fewer features. But instead of simply removing features, we will use a Principal Component Analysis (PCA) to determine the combined features that contribute the most the model (through their accounted variance).\n",
    "\n",
    "**&#9989; Question 4.1 (1 point):** Do a little bit of data preparation before we perform our PCA.\n",
    "\n",
    "Because the features in our dataset have very different ranges of values, the variation captured by the PCA will be skewed by these relative differences. As a result, it is good practice to **normalize** the features so that they have comparable ranges of values. Thankfully, `sklearn` has a useful function for doing this!\n",
    "\n",
    "```from sklearn.preprocessing import MinMaxScaler```\n",
    "\n",
    "Perform a \"Min-Max\" scaling to normalize the features and store the new normalized features in a new dataframe called as `features_norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04602054 0.45693686 0.36430256 ... 0.60265201 0.62007832 0.69713935]\n",
      " [0.09753642 0.07391057 0.45728548 ... 0.31373415 0.24099107 0.25557999]\n",
      " [0.06187733 0.45976209 0.09940156 ... 0.44323887 0.56754043 1.        ]\n",
      " ...\n",
      " [0.00525821 0.50091461 0.47618069 ... 0.53474888 0.59016648 0.35252089]\n",
      " [0.04196893 0.         0.07368592 ... 0.07863614 0.32940815 0.47680055]\n",
      " [0.07706831 0.09233956 0.         ... 0.25596508 0.2701569  0.56331923]]\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "features_norm = scaler.transform(X)\n",
    "print(features_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 4.2 (1 point):** As you did in Question 2.2.1 above, split your new normalized features and corresponding labels (the labels are the same as before) into a training and a testing set, with the training set representing 75% of your data. For reproducibility , set the `random_state` argument to `314159`. Print the lengths to show you have the right number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Training Features: 445\n",
      "# of Test Features: 149\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(features_norm,train_labels,test_size = .25, random_state = 314159)\n",
    "print('# of Training Features:',len(train_vectors))\n",
    "print('# of Test Features:',len(test_vectors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 4.3 (3 points):** Run a Principle Component Analysis (PCA)\n",
    "\n",
    "Since we only have 10 features to start with, let's see how well we can do if we try to aggressively reduce the feature count and use only **3** principle components. We'll see how well we can predict the labels of dataset with just three!\n",
    "\n",
    "\n",
    "(1) Using `PCA()` and the associated `fit()` method, run a principle component analysis on your training features using only 3 components. \n",
    "\n",
    "(2) Transform both the test and training features using the result of your PCA. \n",
    "\n",
    "(3) Print the `explained_variance_ratio_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Explained Variance Ratio: [0.30581474 0.24160577 0.18001823]\n",
      "These 3 components account for a total of 72 percent of the total variance in the original dataset\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "pca = PCA(n_components = 3)\n",
    "_ = pca.fit(train_vectors)\n",
    "\n",
    "pca_train_vectors = _.transform(train_vectors)\n",
    "pca_test_vectors = _.transform(test_vectors)\n",
    "\n",
    "\n",
    "print('PCA Explained Variance Ratio:', pca.explained_variance_ratio_)\n",
    "total_variance = np.sum(pca.explained_variance_ratio_)*100\n",
    "print(\"These 3 components account for a total of %d percent of the total variance in the original dataset\"\n",
    "      % (total_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.4 (1 point):** What is the total explained variance ratio captured by the 3 principle components? (just quote the number) How well do you think a model with these many features will perform? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> 72%. I think this is a decent model. If this few number of components accounted for 100% of variability in the data, I'd be concerned about overfitting with my model. As it stands, it looks like the first three features account for the majority of the variance, with the remaining variance accounted for with the rest of the features. This makes sense, and it would produce a plot with a nice concave shape that we would want here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 4\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Support vector machine based on PCA (14 points)\n",
    "\n",
    "### 5.1 Support vector machine (6 points)\n",
    "\n",
    "For this part, you will build SVC model using the 3 components from PCA, and do grid search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 5.1.1 (2 points):** Build a linear SVC model with `C=0.1`, and fit it to the training set (using the 3 PCA components from the training set).\n",
    "\n",
    "Then use the test features to predict the labels for the testing set. \n",
    "\n",
    "Evaluate the model's performance using the **confusion matrix** and **classification report**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.60501552e-01 -4.87211535e-02 -5.79676356e-02  2.28292690e-01\n",
      "  4.62620799e-02 -6.05401955e-02  1.85956816e-01 -3.36627525e-01\n",
      " -2.95392401e-01  1.60049421e-01 -1.02087096e-01  9.72802376e-02\n",
      "  1.19675918e-01  2.63114402e-01 -1.11971185e-02 -3.26273513e-01\n",
      "  3.83149386e-01 -4.58743161e-02  1.03345572e-01  9.35088650e-02\n",
      " -1.37710995e-01  9.88207232e-02  4.75991984e-02 -3.95367008e-01\n",
      " -5.47843671e-01 -1.75072048e-01 -3.84774287e-01 -1.34842438e-01\n",
      "  5.31226892e-03 -6.48414781e-02  5.20683935e-01 -5.55957009e-03\n",
      " -1.51472320e-01 -2.46009146e-01  1.11385447e-01  5.23630806e-01\n",
      "  4.64295211e-01 -7.23399846e-02 -2.69340488e-01  1.52202115e-01\n",
      " -1.94520776e-01 -4.76737275e-02 -8.00923940e-02  2.95449762e-01\n",
      "  1.99690878e-01 -1.62407517e-01  1.36614181e-01  2.12617740e-02\n",
      " -1.06998166e-02  4.44234630e-01 -4.75275068e-02 -1.14928131e-01\n",
      " -1.59846087e-01  3.73938564e-02  6.50394223e-02  2.24346521e-01\n",
      " -3.07608731e-01  2.77347014e-01  4.09765972e-01  2.17324464e-02\n",
      "  2.42600577e-01  4.80157509e-02  4.72189151e-02  1.86015973e-01\n",
      "  1.87999792e-01 -5.76202020e-02  3.12670792e-01  5.83038505e-01\n",
      " -2.83295945e-01 -6.47611657e-02 -3.24650238e-01  5.77848965e-01\n",
      " -2.80136271e-01  8.44425701e-01  7.78557083e-01  3.21519393e-01\n",
      " -1.51533940e-01 -3.38967434e-01  8.67189601e-02 -1.39173756e-01\n",
      "  3.51213830e-01 -4.42168905e-02  1.19637082e-01 -4.24591502e-01\n",
      "  4.49194512e-01  3.19661621e-02 -4.23172524e-01 -5.28180708e-02\n",
      " -3.20050542e-02 -3.59288346e-01  2.82794498e-01 -1.84924258e-01\n",
      "  6.80238417e-02 -9.38852928e-02 -6.59430596e-02 -4.83936914e-01\n",
      " -4.36976523e-01 -1.60188028e-01  3.03211971e-01 -3.28901927e-02\n",
      " -3.02838526e-01  3.69887960e-02  3.46380876e-01  6.62957623e-02\n",
      " -5.64383711e-01 -7.87631810e-02 -3.03856999e-01  5.02335928e-01\n",
      " -7.06326800e-02  1.30943335e-01  2.72980435e-01 -3.61599631e-01\n",
      " -6.48041714e-01 -1.66357908e-01  3.25909504e-01 -4.06274231e-01\n",
      " -2.36408866e-01  3.58793055e-01  1.93275868e-01 -3.11486603e-01\n",
      "  7.70471778e-01  4.46047534e-01 -4.02979894e-01  5.46141474e-01\n",
      "  3.22653437e-01  1.47979079e-01 -4.07632936e-01 -2.08050141e-01\n",
      " -1.90552624e-01 -3.48293940e-01 -2.18888707e-01  1.39235332e-01\n",
      " -6.69726443e-02  5.02457778e-02 -9.78679373e-02 -7.40229202e-02\n",
      " -3.24057647e-01  1.69109617e-01 -1.47210685e-01  1.06965644e-02\n",
      "  4.58458969e-01  6.75648237e-01  3.30315264e-01 -1.94444257e-01\n",
      " -1.32708819e-01 -2.40613578e-01  5.74420448e-03 -1.34995749e-02\n",
      "  8.57817816e-02 -5.30256463e-01  6.35420004e-01  3.50085444e-01\n",
      " -3.41416515e-01 -2.95868543e-01 -1.24783561e-01 -1.94622898e-01\n",
      " -3.59159325e-01  3.09271902e-01  4.13437603e-01  7.67988177e-02\n",
      "  4.29697140e-01  1.82181758e-01 -3.21750218e-01 -2.60830703e-01\n",
      "  3.83555514e-01 -5.63648475e-01 -3.42300880e-01 -3.73250692e-01\n",
      "  2.43492536e-01 -5.09072640e-02 -4.34430219e-01 -3.44722114e-01\n",
      " -6.59915448e-02  2.22252919e-01  2.87116589e-01  3.79726107e-01\n",
      "  2.02391541e-01  2.17280150e-01 -4.03765382e-02 -1.58962374e-01\n",
      " -1.79292098e-01  4.26913062e-01 -3.47313493e-01  4.86176810e-01\n",
      " -4.15511889e-01 -2.96943614e-01 -2.39863637e-01  2.64847534e-01\n",
      " -1.10202300e-01 -1.69039489e-01  1.83765900e-01  4.00100974e-01\n",
      "  8.09135114e-02 -2.21620800e-01 -6.64182038e-02  3.13600939e-01\n",
      "  8.85194527e-02 -5.64831350e-02 -4.95332325e-01 -2.83386588e-01\n",
      " -1.01399950e-01 -3.26710270e-01 -2.76361424e-01 -5.75518552e-01\n",
      "  1.47952384e-01 -1.26912068e-01  2.63994117e-01 -5.42583713e-02\n",
      "  1.63812518e-01 -3.91696395e-02  3.82410879e-01  8.33212072e-02\n",
      "  4.43757946e-02  1.52297921e-01 -1.36231655e-01 -1.16345135e-01\n",
      " -1.65913875e-01 -4.28585722e-04  5.39533669e-01  8.31890222e-02\n",
      "  7.30166197e-02 -4.04489299e-01 -2.32129769e-01  1.52187618e-01\n",
      "  2.92564775e-01  2.01687384e-01 -1.43594702e-01 -1.82997612e-01\n",
      " -4.64814647e-01 -4.05590768e-01 -8.30503808e-02 -2.23018842e-01\n",
      " -2.44311103e-01 -1.60642470e-01  1.24468627e-01  2.18743552e-01\n",
      "  1.50647293e-01 -9.61811115e-02  1.44410245e-01  2.87870356e-01\n",
      " -3.44778557e-01 -5.88446730e-02  3.00192456e-02 -2.69474022e-01\n",
      "  2.36686171e-01  3.14624280e-01 -2.45971885e-01 -5.35174062e-01\n",
      " -5.07892822e-01 -3.17059170e-01  2.64311108e-02 -2.76379362e-01\n",
      "  3.65329302e-01  3.86205532e-01  8.54848827e-01  2.93838546e-02\n",
      " -1.18737465e-01 -3.56925641e-01 -2.26622082e-01 -9.99039881e-03\n",
      "  1.79237629e-01  4.47823024e-01 -1.45924183e-01  1.72810997e-01\n",
      " -1.70979818e-01 -2.38615977e-02 -1.03819707e-01 -3.51117811e-01\n",
      "  6.66076590e-01 -4.02179628e-01  3.59583789e-01 -8.63378347e-02\n",
      " -2.32961452e-01  3.48029378e-02  6.91394279e-03 -2.76426450e-01\n",
      "  4.74518487e-01  1.66669750e-01  4.09355648e-01 -7.55858732e-02\n",
      "  6.00898204e-02 -2.39387662e-01 -1.49790142e-01  2.96605701e-01\n",
      "  4.09848160e-03 -2.39991880e-01  2.59291765e-01  7.97642775e-01\n",
      " -3.34782641e-02  2.50584956e-01 -1.60251259e-01 -2.78453468e-02\n",
      " -3.91936937e-01  8.05829436e-02  1.11189364e-01 -1.84654035e-01\n",
      "  9.43309230e-02  1.43936059e-01 -2.93714108e-02 -3.47014415e-01\n",
      " -1.23170141e-01  3.10620387e-01  1.37389211e-02  5.83352886e-02\n",
      "  5.36354427e-02  1.07093720e-01  3.69245032e-01  7.09358735e-03\n",
      " -9.88017353e-02 -2.90195872e-01 -2.39157443e-01  1.99571206e-01\n",
      " -4.87042840e-01 -4.82924950e-01 -4.03052594e-01 -3.09821500e-01\n",
      " -7.89785519e-02  1.89548909e-01  6.53084438e-02 -2.23731703e-01\n",
      "  1.00280785e-01  1.57865283e-01 -2.32723001e-01  3.02675261e-01\n",
      "  9.03032228e-02  3.79650933e-02  6.45885480e-02 -2.00781434e-01\n",
      " -3.69861345e-01  1.19971961e-01 -1.93480102e-02 -2.33762436e-01\n",
      " -2.91197708e-01 -3.12804998e-03  3.17805760e-03 -4.03735908e-01\n",
      " -5.31863683e-03  3.09932073e-01 -3.39099828e-01  3.06910583e-01\n",
      "  2.44595387e-01 -1.46263079e-01 -9.27533897e-02 -2.06233337e-02\n",
      "  2.74360054e-02  3.52948474e-01 -4.86376632e-01  2.02408645e-01\n",
      " -4.32058590e-01 -3.09783383e-01  1.16349878e-01  2.21382925e-01\n",
      " -1.46316017e-01  1.94583558e-01  3.42749449e-01  2.82251557e-01\n",
      "  3.32235723e-01  2.53485655e-01 -1.75152656e-01  7.45956808e-02\n",
      " -3.47196916e-01 -8.12979425e-02 -2.97683678e-01 -1.56488903e-01\n",
      "  2.88923153e-02 -2.00188410e-01 -8.87253781e-02 -4.25326684e-01\n",
      "  3.39957714e-01  8.16968921e-02  3.17396550e-01 -9.79695322e-02\n",
      "  5.78814671e-02  1.51142520e-01  4.35769051e-01 -9.41854497e-02\n",
      "  6.39500976e-02 -3.44296631e-02 -7.90783453e-02  1.07594436e-01\n",
      "  5.04812972e-01  2.80856937e-02 -2.60310132e-01  2.16324855e-01\n",
      " -3.04157002e-01 -6.34028045e-02  5.35872120e-01 -4.12675785e-01\n",
      " -1.79271824e-01  6.46210517e-01  2.29774484e-02 -1.35608077e-01\n",
      " -3.26548916e-01 -2.37390388e-01  7.46153825e-02  2.23337442e-01\n",
      " -4.96573875e-01 -3.62431639e-01  7.73397850e-01 -1.48635890e-01\n",
      "  6.65850203e-02  4.41147678e-02 -1.91321459e-01  8.05419767e-01\n",
      " -3.37463590e-01 -2.72236630e-01  1.30617872e-01  4.03850558e-01\n",
      "  2.30441909e-01 -4.22540924e-01  6.36550444e-01 -3.41963834e-01\n",
      "  4.83219559e-02 -1.20442773e-02 -3.45073822e-01 -6.03432926e-02\n",
      " -5.55917465e-01  3.47204913e-01 -4.37175002e-01 -4.24954109e-02\n",
      "  4.73245206e-01 -5.65181734e-02 -3.73108037e-01  2.98215842e-01\n",
      "  5.10263436e-02 -3.39920744e-02  5.48025241e-02  1.27236832e-02\n",
      "  3.65325511e-01 -4.26090004e-01  3.57807118e-01 -3.92464131e-02\n",
      " -2.05301986e-01  4.32412238e-02  2.70842392e-01  1.45230698e-01\n",
      "  3.04062479e-01 -3.55224672e-01  4.70784378e-01 -1.68124618e-01\n",
      "  3.24025197e-01  4.14231085e-01 -1.11782075e-01 -4.01674991e-01\n",
      " -7.97492899e-02]\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(pca_train_vectors,train_labels,test_size = .25, random_state = 314159)\n",
    "print(pca_train_vectors[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 5.1.2 (3 points):** Find the best hyperparameters\n",
    "\n",
    "At this point, we have fit one SVC model and determined it's performance, but is it the best model? We can use `GridSearchCV` to find the best model (given our choices of parameters). Once we do that, we will use that \"best\" model for making predictions.\n",
    "\n",
    "Using the following parameters (`C` = `1e-3`, `0.01`, `0.1`, `1`, `10`, `100` and `gamma` = `1e-6`, `1e-5`, `1e-4`, `1e-3`, `0.01`, `0.1`) for both a `linear` and `rbf` kernel use `GridSearchCV` with the `SVC()` model to find the best fit parameters. Once, you've run the grid search, print the \"best estimators\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "Best estimator found by grid search:\n",
      "SVC(C=100, class_weight='balanced', gamma=0.1)\n",
      "Best parameters found by grid search:\n",
      "{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "tmp_vectors = train_vectors\n",
    "tmp_labels = train_labels\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "\n",
    "param_grid = {'C': [1e-3, 0.01, 0.1, 1.0, 10,100],\n",
    "              'gamma': [1e-6,1e-5,1e-4,1e-3,0.01,0.1],\n",
    "              'kernel': ['linear','rbf']}\n",
    "\n",
    "clf = GridSearchCV(SVC(class_weight='balanced'), param_grid)\n",
    "\n",
    "clf = clf.fit(tmp_vectors, tmp_labels)\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 5.1.3 (1 point):**  Evaluate the best fit model\n",
    "\n",
    "Now that we have found the \"best estimators\", let's determine how good the fit is.\n",
    "\n",
    "Use the test features to predict the labels, based on the best model. Evaluate the performance using the **confusion matrix** and **classification report**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting names on the test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.48      0.51        56\n",
      "           1       0.53      0.59      0.56        56\n",
      "\n",
      "    accuracy                           0.54       112\n",
      "   macro avg       0.54      0.54      0.53       112\n",
      "weighted avg       0.54      0.54      0.53       112\n",
      "\n",
      "[[27 29]\n",
      " [23 33]]\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "predict_vectors = test_vectors\n",
    "true_labels = test_labels\n",
    "\n",
    "print(\"Predicting names on the test set\")\n",
    "pred_labels = clf.predict(predict_vectors)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "print(confusion_matrix(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 How well does PCA work? (8 points)\n",
    "The number of components we use in our PCA matters. Let's investigate how they matter by systematically building a model for any number of selected components. While this might seem a bit unnecessary for such a simple dataset, **this can be very useful for more complex datasets and models!**\n",
    "\n",
    "**&#9989; Question 5.2.1 (3 points):**\n",
    "\n",
    "To systematically explore how well PCA improves our classification model, we will do this by writing a function that \n",
    "* creates the PCA\n",
    "* creates the SVC model\n",
    "* uses `GridSearchCV` to find the best hyperparameters\n",
    "* predicts the labels using test data\n",
    "* returns the accuracy scores and the explained variance ratio.\n",
    "\n",
    "Just as you did in Question 5.1.2, use the following parameters (`C` = `1e-3`, `0.01`, `0.1`, `1`, `10`, `100` and `gamma` = `1e-6`, `1e-5`, `1e-4`, `1e-3`, `0.01`, `0.1`) for both a `linear` and `rbf` kernel use `GridSearchCV` with the `SVC()` model to find the best fit parameters.\n",
    "\n",
    "So, Your function will take as input:\n",
    "* the number of requested PCA components\n",
    "* the training feature data\n",
    "* the testing feature data\n",
    "* the training data labels\n",
    "* the test data labels\n",
    "\n",
    "and it should **return** the accuracy score for an SVC model fit to pca transformed features and the **total** explained variance ratio (i.e. the sum of the explained variance for each component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "\n",
    "def reduced_SVM(n_components, train_features, train_labels, test_features, test_labels):\n",
    "\n",
    "    pca = PCA(n_components)\n",
    "    _ = pca.fit(train_vectors)\n",
    "\n",
    "    pca_train_vectors = _.transform(train_vectors)\n",
    "    pca_test_vectors = _.transform(test_vectors)\n",
    "    \n",
    "    param_grid = {'C': [1e-3, 0.01, 0.1, 1.0, 10,100],\n",
    "              'gamma': [1e-6,1e-5,1e-4,1e-3,0.01,0.1],\n",
    "              'kernel': ['linear','rbf']}\n",
    "    # make a classifier by searching over a classifier and the parameter grid\n",
    "    clf = GridSearchCV(SVC(class_weight='balanced'), param_grid)\n",
    "\n",
    "    # we have a \"good\" classifier (according to GridSearchCV), how's it look\n",
    "    clf = clf.fit(pca_train_vectors, train_labels)\n",
    "    \n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(clf.best_estimator_)\n",
    "    print(\"Best parameters found by grid search:\")\n",
    "    print(clf.best_params_)\n",
    "    \n",
    "    print('PCA Explained Variance Ratio', i, 'components:', pca.explained_variance_ratio_)\n",
    "    total_variance = np.sum(pca.explained_variance_ratio_)*100\n",
    "    print(\"These components account for a total of %d percent of the total variance in the dataset\"\n",
    "          % (total_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 5.2.2 (2 points):**\n",
    "\n",
    "Now that you have created a function that returns the accuracy for a given number of components, we will use that to plot the how the accuracy of your SVC model changes when we increase the number of components used in the PCA.\n",
    "\n",
    "For 1 through 10 components, use your function above to compute and store (as a list) the accuracy of your models and the total explained variance ratio of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator found by grid search:\n",
      "SVC(C=10, class_weight='balanced', gamma=0.1)\n",
      "Best parameters found by grid search:\n",
      "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "PCA Explained Variance Ratio 1 components: [0.42241544]\n",
      "These components account for a total of 42 percent of the total variance in the dataset\n",
      "Best estimator found by grid search:\n",
      "SVC(C=10, class_weight='balanced', gamma=0.1)\n",
      "Best parameters found by grid search:\n",
      "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "PCA Explained Variance Ratio 2 components: [0.42241544 0.3414825 ]\n",
      "These components account for a total of 76 percent of the total variance in the dataset\n",
      "Best estimator found by grid search:\n",
      "SVC(C=100, class_weight='balanced', gamma=0.1)\n",
      "Best parameters found by grid search:\n",
      "{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "PCA Explained Variance Ratio 3 components: [0.42241544 0.3414825  0.23610206]\n",
      "These components account for a total of 99 percent of the total variance in the dataset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=4 must be between 0 and min(n_samples, n_features)=3 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-88c63c7f3f3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduced_SVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-e87063407fa1>\u001b[0m in \u001b[0;36mreduced_SVM\u001b[0;34m(n_components, train_features, train_labels, test_features, test_labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpca_train_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    437\u001b[0m                                  \"if n_samples >= n_features\")\n\u001b[1;32m    438\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             raise ValueError(\"n_components=%r must be between 0 and \"\n\u001b[0m\u001b[1;32m    440\u001b[0m                              \u001b[0;34m\"min(n_samples, n_features)=%r with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                              \u001b[0;34m\"svd_solver='full'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=4 must be between 0 and min(n_samples, n_features)=3 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "for i in np.arange(1,10):\n",
    "    i = reduced_SVM(i,train_vectors, train_labels, test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I get an error here because my function accounts for all the variance with only the first three principal components. The only think I can think of is that somewhere I input my test data, so I'm fitting to the test data instead of the train data. So my function and for loop is doing what it is supposed to do here, but my mistake somewhere else is causing me issues here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 5.2.3 (1 point):** Plot the accuracy vs # of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [445, 333]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-73573e9f61ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Put your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_train_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [445, 333]"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "plt.plot(accuracy_score(pca_train_vectors, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 5.2.4 (1 point):** Where does it seem like we have diminishing returns? That is, at what point is there no major increase in accuracy (or perhaps the accuracy is decreased) as we add additional components to the PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Erase this and put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Task 5.2.5 (1 point):** Plot the total explained variance ratio vs # of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98c462d190>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put your code here\n",
    "plt.plot(pca.explained_variance_ratio_, marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 5\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment wrap-up¶\n",
    "Please fill out the form that appears when you run the code below. **You must completely fill this out in order to receive credit for the assignment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe \n",
       "\tsrc=\"https://forms.office.com/Pages/ResponsePage.aspx?id=MHEXIi9k2UGSEXQjetVofa-byNJHa0xBs0jOGcRl02lURU83U0ZHUUpWUUFRUzhCQ0JZWDQxVVRUVi4u\" \n",
       "\twidth=\"800px\" \n",
       "\theight=\"600px\" \n",
       "\tframeborder=\"0\" \n",
       "\tmarginheight=\"0\" \n",
       "\tmarginwidth=\"0\">\n",
       "\tLoading...\n",
       "</iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\n",
    "\"\"\"\n",
    "<iframe \n",
    "\tsrc=\"https://forms.office.com/Pages/ResponsePage.aspx?id=MHEXIi9k2UGSEXQjetVofa-byNJHa0xBs0jOGcRl02lURU83U0ZHUUpWUUFRUzhCQ0JZWDQxVVRUVi4u\" \n",
    "\twidth=\"800px\" \n",
    "\theight=\"600px\" \n",
    "\tframeborder=\"0\" \n",
    "\tmarginheight=\"0\" \n",
    "\tmarginwidth=\"0\">\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you're done!\n",
    "Submit this assignment by uploading it to the course Desire2Learn web page. Go to the \"Homework Assignments\" folder, find the submission folder for Homework #5, and upload your notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
